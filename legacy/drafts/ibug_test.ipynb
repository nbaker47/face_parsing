{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8OaZQL5Lk1Fo"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights, lraspp_mobilenet_v3_large, deeplabv3_resnet101\n",
        "import label_test_script\n",
        "from label_test_script import visualize, reverse_one_hot, colour_code_segmentation\n",
        "\n",
        "#Hyperparamters\n",
        "ENCODER = 'resnet101'\n",
        "ENCODER_WEIGHTS = 'imagenet' #pretrained weighting\n",
        "#CLASSES = [\"background\", \"skin\", \"nose\", \"right_eye\", \"left_eye\", \"right_brow\", \"left_brow\", \"right_ear\", \"left_ear\", \"mouth_interior\", \"top_lip\", \"bottom_lip\", \"neck\", \"hair\", \"beard\", \"clothing\", \"glasses\", \"headwear\", \"facewear\"]\n",
        "ACTIVATION = \"sigmoid\" # softmax2d for multiclass segmentation\n",
        "num_classes = 11\n",
        "\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dlib\n",
        "import glob\n",
        "\n",
        "datFile = \"/home/nathan/Documents/final_project/shape_predictor_5_face_landmarks.dat\"\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(datFile)  \n",
        "\n",
        "SIZEX = 128\n",
        "SIZEY = 128\n",
        "\n",
        "def shape_to_normal(shape):\n",
        "  shape_normal = []\n",
        "  for i in range(0, 5):\n",
        "      shape_normal.append((i, (shape.part(i).x, shape.part(i).y)))\n",
        "  return shape_normal\n",
        "\n",
        "def get_eyes_nose_dlib(shape):\n",
        "    nose = shape[4][1]\n",
        "    left_eye_x = int(shape[3][1][0] + shape[2][1][0]) // 2\n",
        "    left_eye_y = int(shape[3][1][1] + shape[2][1][1]) // 2\n",
        "    right_eyes_x = int(shape[1][1][0] + shape[0][1][0]) // 2\n",
        "    right_eyes_y = int(shape[1][1][1] + shape[0][1][1]) // 2\n",
        "    return nose, (left_eye_x, left_eye_y), (right_eyes_x, right_eyes_y)\n",
        "\n",
        "def distance(a, b):\n",
        "    return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
        "\n",
        "def cosine_formula(length_line1, length_line2, length_line3):\n",
        "  cos_a = -(length_line3 ** 2 - length_line2 ** 2 - length_line1 ** 2) / (2 * length_line2 * length_line1)\n",
        "  return cos_a\n",
        "\n",
        "def rotate_point(origin, point, angle):\n",
        "    ox, oy = origin\n",
        "    px, py = point\n",
        "\n",
        "    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)\n",
        "    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)\n",
        "    return qx, qy\n",
        "\n",
        "def is_between(point1, point2, point3, extra_point):\n",
        "    c1 = (point2[0] - point1[0]) * (extra_point[1] - point1[1]) - (point2[1] - point1[1]) * (extra_point[0] - point1[0])\n",
        "    c2 = (point3[0] - point2[0]) * (extra_point[1] - point2[1]) - (point3[1] - point2[1]) * (extra_point[0] - point2[0])\n",
        "    c3 = (point1[0] - point3[0]) * (extra_point[1] - point3[1]) - (point1[1] - point3[1]) * (extra_point[0] - point3[0])\n",
        "    if (c1 < 0 and c2 < 0 and c3 < 0) or (c1 > 0 and c2 > 0 and c3 > 0):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def align(img):\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
        "    #face alignment\n",
        "    rects = detector(gray, 0)\n",
        "    if len(rects) > 0:\n",
        "        for rect in rects:\n",
        "            x = rect.left()\n",
        "            y = rect.top()\n",
        "            w = rect.right()\n",
        "            h = rect.bottom()\n",
        "            shape = predictor(gray, rect)\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "    shape = shape_to_normal(shape)\n",
        "    nose, left_eye, right_eye = get_eyes_nose_dlib(shape)\n",
        "\n",
        "    center_of_forehead = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)\n",
        "\n",
        "    center_pred = (int((x + w) / 2), int((y + y) / 2))\n",
        "\n",
        "    length_line1 = distance(center_of_forehead, nose)\n",
        "    length_line2 = distance(center_pred, nose)\n",
        "    length_line3 = distance(center_pred, center_of_forehead)\n",
        "\n",
        "    cos_a = cosine_formula(length_line1, length_line2, length_line3)\n",
        "    angle = np.arccos(cos_a)\n",
        "\n",
        "    rotated_point = rotate_point(nose, center_of_forehead, angle)\n",
        "    rotated_point = (int(rotated_point[0]), int(rotated_point[1]))\n",
        "    if is_between(nose, center_of_forehead, center_pred, rotated_point):\n",
        "        angle = np.degrees(-angle)\n",
        "    else:\n",
        "        angle = np.degrees(angle)\n",
        "    \n",
        "    #gray = Image.fromarray(gray)\n",
        "    #gray = np.array(gray.rotate(angle))\n",
        "\n",
        "    return angle\n",
        "\n",
        "def crop_rotate(img):\n",
        "    \"\"\" HAAR CASCADE CLASSIFIER AND ROATER\"\"\"\n",
        "    #face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')\n",
        "\n",
        "    crops = []\n",
        "    original = 0\n",
        "    skips = []\n",
        "    rejected = []\n",
        "    angles = []\n",
        "\n",
        "    #datFile = \"/home/nathan/Documents/final_project/shape_predictor_5_face_landmarks.dat\"\n",
        "\n",
        "    # convert to gray\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    face = None\n",
        "    # Detect faces\n",
        "    try:\n",
        "      face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\")\n",
        "      limit = int((gray.shape[0]/2))\n",
        "      faces = face_cascade.detectMultiScale(gray, 1.05, 2, minSize=[limit,0])\n",
        "      face = sorted(faces,key=lambda f:f[2]*f[3])[-1]\n",
        "    except:\n",
        "      #print(\"FAILED USING FRONTAL FACE ALT 2\")\n",
        "      try:\n",
        "        face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
        "        limit = int((gray.shape[0]/2))\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.05, 2, minSize=[limit,0])\n",
        "        face = sorted(faces,key=lambda f:f[2]*f[3])[-1]\n",
        "      except:\n",
        "        try:\n",
        "          face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "          limit_1 = int((gray.shape[0]/2.5))\n",
        "          limit_2 = int((gray.shape[1]/1.7))\n",
        "          faces = face_cascade.detectMultiScale(gray, 1.05, 3, minSize=[limit_1,0], maxSize=[50000,limit_2]) \n",
        "          face = sorted(faces,key=lambda f:f[2]*f[3])[-1]  \n",
        "        except:\n",
        "          #print(\"FAILED USING FRONTAL FACE ALT 2\")\n",
        "          try:\n",
        "            face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "            limit_1 = int((gray.shape[0]/2.5))\n",
        "            limit_2 = int((gray.shape[1]/2.5))\n",
        "            faces = face_cascade.detectMultiScale(gray, minSize=[0,limit_2])\n",
        "            face = sorted(faces,key=lambda f:f[2]*f[3])[-1]\n",
        "          except:\n",
        "            #print(\"FAILED USING FRONTAL FACE ALT 2\")\n",
        "            try:\n",
        "              face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\")\n",
        "              limit_1 = int((gray.shape[0]/2))\n",
        "              limit_2 = int((gray.shape[1]/3))\n",
        "              faces = face_cascade.detectMultiScale(gray, 1.03, 1, minSize=[limit_1,0])\n",
        "              face = sorted(faces,key=lambda f:f[2]*f[3])[-1]\n",
        "            except:\n",
        "              #print(\"FAILED USING PROFILE FACE ALT\")\n",
        "              pass\n",
        "\n",
        "    if face is not None:\n",
        "      x, y, w, h = face\n",
        "      crops.append(face)\n",
        "      face = img[y:y + h, x:x + w]\n",
        "      face = cv2.resize(face, (SIZEY, SIZEX))\n",
        "      #cv2_imshow(face)\n",
        "\n",
        "      angle = align(img)\n",
        "      face = Image.fromarray(face)\n",
        "      face_rotated = np.array(face.rotate(angle))\n",
        "      angles.append(angle)\n",
        "\n",
        "    return (face_rotated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A95DA-azk1Fl"
      },
      "outputs": [],
      "source": [
        "#import torchvision.transforms as T\n",
        "#import torchvision.transforms.functional as F\n",
        "import albumentations as albu\n",
        "import random\n",
        "import scipy\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "#from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "\n",
        "rgb_vals = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "#rgb_vals = [0,1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    x_t = x.transpose(2, 0, 1).astype('float32')\n",
        "    #print(\"XTSHAPE\", x_t.shape)\n",
        "    return x_t\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        #albu.HorizontalFlip(p=0.5),\n",
        "        albu.Rotate((-18,18)),\n",
        "        albu.PadIfNeeded(min_height=SIZEY, min_width=SIZEY, always_apply=True, border_mode=0),\n",
        "        #albu.RandomCrop(height=320, width=320, always_apply=True),\n",
        "        albu.Perspective(p=0.5),\n",
        "        albu.GaussNoise(p=0.2),\n",
        "        albu.OneOf([albu.CLAHE(p=1),albu.RandomBrightness(p=1),albu.RandomGamma(p=1),],p=0.9,),\n",
        "        albu.OneOf([albu.Sharpen(p=1),albu.Blur(blur_limit=3, p=1),],p=0.9,),albu.OneOf([albu.RandomContrast(p=1),albu.HueSaturationValue(p=1),],p=0.9,),\n",
        "    ]\n",
        "\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def transformation_augs():\n",
        "    train_transform = [\n",
        "        #albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(SIZEY, SIZEY)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "\n",
        "  #CLASSES =  [\"background\",\"facial_skin\",\"left_brow\",\"right_brow\",\"left_eye\",\"right_eye\", \"nose\",\"upper_lip\",\"inner_mouth\",\"lower_lip\",\"hair\"]\n",
        "\n",
        "  def __init__(self, images_dir, masks_dir, coords_dir, preprocessing=None, classes=None,augmentation=None, mode=\"train\", use_landmarks=True):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    \n",
        "    # store the augmented tensors tensors\n",
        "    #self._x, self._y = x,y\n",
        "    self.preprocessing = preprocessing\n",
        "    self.augmentation = augmentation\n",
        "\n",
        "    self.image_ids = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    self.use_landmarks = use_landmarks\n",
        "\n",
        "    if mode == \"val\":\n",
        "      self.masks_fps = [os.path.join(images_dir, image_id) for image_id in self.image_ids]\n",
        "      self.images_fps = [os.path.join(masks_dir, mask_id.replace(\"jpg\",\"png\")) for mask_id in self.image_ids]\n",
        "      self.coords_fps = [os.path.join(coords_dir, coords_id.replace(\".jpg\", \"_landmark.txt\")) for coords_id in self.image_ids]\n",
        "\n",
        "  def __len__(self):\n",
        "    # a DataSet must know it size\n",
        "    return len(self.images_fps)\n",
        "\n",
        "  def __getitem__(self, i, put_back=False):\n",
        "\n",
        "    #print(self.masks_fps[i], self.images_fps[i])\n",
        "\n",
        "    image = cv2.imread(self.masks_fps[i])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.imread(self.images_fps[i],0)\n",
        "\n",
        "    # crop to center face\n",
        "    if self.use_landmarks == True:\n",
        "      smallest_x = 99999\n",
        "      smallest_y = 99999\n",
        "      biggest_x = -99999\n",
        "      biggest_y = -99999\n",
        "      with open(self.coords_fps[i], 'rb') as f:\n",
        "\n",
        "        contents = str(f.read()).split(\"\\\\n\")\n",
        "        #print(contents)\n",
        "        contents = [[int(float(single.replace(\"\\\\r\", \"\").\n",
        "                              replace(\"'\", \"\").replace(\"b\", \"\").replace(\"\\\\x1a\", \"\")\n",
        "                              )) for single in pair.split(\" \")] for pair in contents[2:-2]]\n",
        "        #print(contents)\n",
        "\n",
        "        #contents = np.array(contents)\n",
        "        for pair in contents:\n",
        "          #plt.scatter((pair[0]), (pair[1]), color=\"red\")\n",
        "\n",
        "          if (pair[0]) < smallest_x:\n",
        "            smallest_x = (pair[0])\n",
        "          \n",
        "          if (pair[0]) > biggest_x:\n",
        "            biggest_x = (pair[0])\n",
        "\n",
        "          if (pair[1]) < smallest_y:\n",
        "            smallest_y = (pair[1])\n",
        "          \n",
        "          if (pair[1]) > biggest_y:\n",
        "            biggest_y = (pair[1])\n",
        "\n",
        "        #save original for repositioning\n",
        "        crop_coords = (int(smallest_y), int(biggest_y), int(smallest_x), int(biggest_x))\n",
        "        original_image = image\n",
        "        original_mask = mask\n",
        "\n",
        "        #crop to points of ineterst\n",
        "        image = image[smallest_y: biggest_y, smallest_x:biggest_x]\n",
        "        mask = mask[smallest_y: biggest_y, smallest_x:biggest_x]\n",
        "\n",
        "      try:\n",
        "        mask = cv2.resize(mask, (SIZEY, SIZEY))\n",
        "        image = cv2.resize(image, (SIZEY, SIZEY))\n",
        "        mask = np.expand_dims(mask,2)\n",
        "      except:\n",
        "        print(self.masks_fps[i])\n",
        "        print(self.images_fps[i])\n",
        "        plt.imshow(mask)\n",
        "        plt.imshow(image)\n",
        "    \n",
        "    else:\n",
        "      # use haar classifier\n",
        "      crop_and_rotate()\n",
        "\n",
        "    # smooth mask\n",
        "    # mask = mask = scipy.ndimage.median_filter(mask, 5)\n",
        "\n",
        "    # apply augmentations\n",
        "    if self.augmentation:\n",
        "        sample = self.augmentation(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "    \n",
        "    # apply preprocessing\n",
        "    if self.preprocessing:\n",
        "        sample = self.preprocessing(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "\n",
        "    #onehot\n",
        "    one_hot_Y = torch.nn.functional.one_hot(torch.tensor(mask).to(torch.int64), num_classes).permute(0,3,1,2).float().squeeze(0)\n",
        "\n",
        "    if put_back == True:\n",
        "      return (image, one_hot_Y, original_image, original_mask, crop_coords)\n",
        "\n",
        "    else:\n",
        "      return (image, one_hot_Y)\n",
        "\n",
        "  def get_og(self, i):\n",
        "    return self.__getitem__(i, put_back=True)\n",
        "\n",
        "\n",
        "val_img_path = \"/home/nathan/Documents/final_project/datasets/ibugmask_release/test\"\n",
        "val_mask_path = val_img_path\n",
        "val_coord_path= val_img_path\n",
        "\n",
        "val_ds = MyDataSet(val_mask_path,\n",
        "                   val_img_path,\n",
        "                   val_coord_path,\n",
        "                   preprocessing=get_preprocessing(preprocessing_fn),\n",
        "                   augmentation=get_validation_augmentation(), \n",
        "                   mode=\"val\")\n",
        "\n",
        "image_vis, gt_mask = val_ds[20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ug8TWvk0ntQ",
        "outputId": "750fb335-b21e-4ee4-e3d9-03e3076f42fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "154\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(len(val_ds))\n",
        "\n",
        "for x in range(0):\n",
        "\n",
        "  image_vis, gt_mask = val_ds[x]\n",
        "  print(x, \":\", image_vis.shape, gt_mask.shape)\n",
        "\n",
        "  gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "\n",
        "  visualize(\n",
        "      original_image = image_vis[0,::],\n",
        "      ground_truth_mask = gt_mask\n",
        "  )\n",
        "\n",
        "#np.unique(image_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKwG2RgVk1Fn"
      },
      "source": [
        "## Initialise Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FXGkPrD3k1Fo"
      },
      "outputs": [],
      "source": [
        "from segmentation_models_pytorch import utils\n",
        "DEVICE = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqWwaSlFk1Fp"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3WowjSXk1Fq"
      },
      "source": [
        "### Function to view train model precitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "buTwNEGak1Fq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-16 15:57:26.592547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-16 15:57:26.953923: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-02-16 15:57:28.060574: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nathan/miniconda3/lib/python3.9/site-packages/cv2/../../lib64::/home/nathan/miniconda3/lib/:/home/nathan/miniconda3/lib/:/home/nathan/miniconda3/lib/\n",
            "2023-02-16 15:57:28.060706: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nathan/miniconda3/lib/python3.9/site-packages/cv2/../../lib64::/home/nathan/miniconda3/lib/:/home/nathan/miniconda3/lib/:/home/nathan/miniconda3/lib/\n",
            "2023-02-16 15:57:28.060717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/tmp/ipykernel_79967/3169589358.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "import tensorflow as tf\n",
        "from torchmetrics.classification import F1Score, BinaryF1Score, MulticlassF1Score, JaccardIndex\n",
        "from torchmetrics import Dice\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "def average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "def view_predictions(model, ds, numm_classes, amount=-1, visualise=False):\n",
        "\n",
        "    ious = []\n",
        "    f1s = []\n",
        "\n",
        "    if amount == -1:\n",
        "      amount = len(ds)\n",
        "\n",
        "    #predict\n",
        "    for idx in range(amount):\n",
        "\n",
        "        image, gt_mask, og_image, og_mask, coords = ds.get_og(idx)\n",
        "        image = image\n",
        "        image_vis = image\n",
        "        image_vis = np.transpose(image_vis,(1,2,0))\n",
        "        x_tensor = torch.tensor(image).to(DEVICE).unsqueeze(0)\n",
        "        pred_mask = model(x_tensor)\n",
        "        pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "        pred_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(pred_mask)), rgb_vals)\n",
        "        gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "\n",
        "        #plt.imshow(og_image,interpolation='none')\n",
        "        #plt.imshow(gt_mask, alpha=0.2,interpolation='none') # interpolation='none'\n",
        "        #plt.show()\n",
        "        \n",
        "        #get IoU score\n",
        "        m = tf.keras.metrics.MeanIoU(num_classes=numm_classes)\n",
        "        m.update_state(gt_mask, pred_mask)\n",
        "        iou = m.result().numpy()\n",
        "        #print(\"MEAN IoU:\" , iou)\n",
        "        ious.append(iou)\n",
        "\n",
        "        metric = MulticlassF1Score(num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        #metric = JaccardIndex(task=\"multiclass\",num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        f1 = metric(torch.tensor(gt_mask), torch.tensor(pred_mask))\n",
        "        if len(f1) == num_classes:\n",
        "          f1[f1 <0.1] = 0.1\n",
        "          #print(f1)\n",
        "          f1s.append(np.array(f1))\n",
        "\n",
        "        if visualise:\n",
        "          visualize(\n",
        "              img_crop = image.T,\n",
        "              gt_crop= gt_mask,\n",
        "              pred_crop = pred_mask,\n",
        "          )\n",
        "\n",
        "          og_image = og_image\n",
        "          upscale_coords = og_image[coords[0]: coords[1], coords[2]:coords[3]].shape\n",
        "          pred_mask = Image.fromarray(pred_mask.astype(np.uint8)).resize(upscale_coords[:-1][::-1])\n",
        "          pred_mask = cv2.cvtColor(np.array(pred_mask),cv2.COLOR_GRAY2RGB)\n",
        "          pred_mask =Image.fromarray(pred_mask)\n",
        "\n",
        "          [xs,ys]=pred_mask.size  #width*height\n",
        "          colour_dict = {0:(0,0,0), 1:(39,65,135), 2:(70,136,154), 3:(52,158,136), 4:(37,157,97), 5:(23,180,23), \n",
        "                          6:(22,180,23), 7:(113,203,58), 8:(219,213,68), 9:(230,109,11), 10:(255,56,10), 11:(0,0,0), 12:(0,0,0)}\n",
        "                          \n",
        "          # Examine every pixel in im\n",
        "          for x in range(0,xs):\n",
        "            for y in range(0,ys):\n",
        "              #get the RGB color of the pixel\n",
        "              [r,g,b]=pred_mask.getpixel((x, y))\n",
        "              value = colour_dict[r]\n",
        "              pred_mask.putpixel((x, y), value)\n",
        "          \n",
        "          print(np.array(pred_mask).shape, og_image.shape, upscale_coords)\n",
        "\n",
        "          pred_mask = np.array(pred_mask)\n",
        "          for x, rows in enumerate(pred_mask):\n",
        "            for y, columns in enumerate(rows):\n",
        "              r = pred_mask[x,y][0]\n",
        "              if r != 0:\n",
        "\n",
        "                og_image[x+int(coords[0]),y+int(coords[2])] = pred_mask[x,y]\n",
        "              else:\n",
        "                pass\n",
        "\n",
        "          plt.imshow(og_image,interpolation='none')\n",
        "          #plt.imshow(gt_mask, alpha=0.2,interpolation='none') # interpolation='none'\n",
        "          plt.show()\n",
        "\n",
        "        print(idx)\n",
        "\n",
        "    \n",
        "    fs1_numpy = np.array(f1s)\n",
        "    av_f1s = np.nanmean(fs1_numpy, axis=0)\n",
        "    #av_f1s = fs1_numpy.mean(axis=0)\n",
        "    av_f1s_av = av_f1s.mean(axis=0)\n",
        "\n",
        "    print (\"Dataset MIoU = \", average(ious))\n",
        "    print (\"Dataset F1 = \", av_f1s)\n",
        "    print (\"Dataset F1 av = \", av_f1s_av)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iUjn7DVPBcr0"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "def save_predictions(model, ds):\n",
        "\n",
        "  xs = []\n",
        "  ys = []\n",
        "\n",
        "  #predict\n",
        "  for idx in range(len(ds)):\n",
        "\n",
        "      # Pop image from DS\n",
        "      image, gt_mask = ds[idx]\n",
        "      image_vis = image\n",
        "      image_vis = np.transpose(image_vis,(1,2,0))\n",
        "      \n",
        "      # Reshape\n",
        "      x_tensor = torch.tensor(image).to(DEVICE).unsqueeze(0)\n",
        "      # Predict test image\n",
        "      pred_mask = model(x_tensor)\n",
        "      # Reshape\n",
        "      pred_mask = pred_mask.detach().squeeze().cpu()\n",
        "\n",
        "      print(\"saving\", idx, \"/\", len(ds))\n",
        "\n",
        "      # Save Predictions for use in Label Adapter\n",
        "      xs.append(pred_mask)\n",
        "\n",
        "      ys.append(gt_mask)\n",
        "      \n",
        "  return xs, ys\n",
        "\n",
        "#save_predictions(model, val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD5uZhW8k1Fq"
      },
      "source": [
        "# Predict with Deeplabv3+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_79967/3169589358.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
            "2023-02-16 15:57:33.766146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:33.773399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:33.773659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:33.774231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-16 15:57:33.774912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:33.775221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:33.775411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:34.319914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:34.320188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:34.320411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-02-16 15:57:34.320582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1574 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "Dataset MIoU =  0.4504468680976273\n",
            "Dataset F1 =  [0.7040327  0.8971815  0.41992015 0.43955752 0.47745544 0.47638896\n",
            " 0.8513073  0.5610244  0.51092225 0.6255943  0.40041447]\n",
            "Dataset F1 av =  0.5785272\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "Dataset MIoU =  0.4644759959214694\n",
            "Dataset F1 =  [0.73702866 0.91099083 0.45246723 0.47828302 0.4871554  0.11962242\n",
            " 0.85583824 0.61927295 0.553059   0.66718334 0.570865  ]\n",
            "Dataset F1 av =  0.58652425\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "Dataset MIoU =  0.4814616446564724\n",
            "Dataset F1 =  [0.74118775 0.9077558  0.44240373 0.47322208 0.5147659  0.5207166\n",
            " 0.857679   0.61319804 0.55283445 0.65487367 0.4077798 ]\n",
            "Dataset F1 av =  0.6078561\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "Dataset MIoU =  0.49894240421134156\n",
            "Dataset F1 =  [0.73893505 0.9132816  0.44976896 0.4737803  0.5181837  0.51532936\n",
            " 0.8580242  0.62143487 0.5497568  0.6729432  0.5828311 ]\n",
            "Dataset F1 av =  0.6267518\n"
          ]
        }
      ],
      "source": [
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/deeplab.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes, amount=-1, visualise=False)\n",
        "\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/fcn.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes, amount=-1, visualise=False)\n",
        "\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/unet.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes, amount=-1, visualise=False)\n",
        "\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/mobile.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes, amount=-1, visualise=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTKVQT5ecjtP"
      },
      "source": [
        "### Load and view model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "VycUDUz-VRaF",
        "outputId": "4199ca6b-b388-49b7-a529-8afe38b5c690"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nprint(\"MIXED:\")\\nprint(\"deep\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/deeplab.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"FCN\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/fcn.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"mobile\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/mobile.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"unet\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/unet.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\n\\n\\nprint(\"HELEN:\")\\nprint(\"deep\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/deeplab.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"FCN\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/fcn.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"mobile\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/mobile.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"unet\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/unet.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\n\\n\\nprint(\"LAPA:\")\\nprint(\"deep\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/deeplab.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"FCN\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/fcn.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"mobile\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/mobile.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\nprint(\"unet\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/unet.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, num_classes)\\n\\n\\nprint(\"SYNTH:\")\\nprint(\"deep\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/deeplab.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, 12)\\n\\nprint(\"FCN\")\\nmodel = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/fcn.pth\", map_location=DEVICE)\\nview_predictions(model,val_ds, 12)\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "print(\"MIXED:\")\n",
        "print(\"deep\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/deeplab.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"FCN\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/fcn.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"mobile\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/mobile.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"unet\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/mixed/unet.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "print(\"HELEN:\")\n",
        "print(\"deep\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/deeplab.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"FCN\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/fcn.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"mobile\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/mobile.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"unet\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/helen/unet.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "print(\"LAPA:\")\n",
        "print(\"deep\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/deeplab.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"FCN\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/fcn.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"mobile\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/mobile.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "print(\"unet\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/lapa/unet.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, num_classes)\n",
        "\n",
        "\n",
        "print(\"SYNTH:\")\n",
        "print(\"deep\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/deeplab.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, 12)\n",
        "\n",
        "print(\"FCN\")\n",
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/fcn.pth\", map_location=DEVICE)\n",
        "view_predictions(model,val_ds, 12)\n",
        "\"\"\"\n",
        "#print(\"mobile\")\n",
        "#model = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/mobile.pth\", map_location=DEVICE)\n",
        "#view_predictions(model,val_ds, 12)\n",
        "\n",
        "#print(\"unet\")\n",
        "#model = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/unet.pth\", map_location=DEVICE)\n",
        "#view_predictions(model,val_ds, 12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ZyVrbFB_lI"
      },
      "source": [
        "# label adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zy9r-A2L2JmF"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_ds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xs, ys \u001b[38;5;241m=\u001b[39m save_predictions(model, train_ds)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ],
      "source": [
        "xs, ys = save_predictions(model, train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RBSurf5YymN"
      },
      "outputs": [],
      "source": [
        "len(xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYbvP4uHXaa5"
      },
      "outputs": [],
      "source": [
        "for x in range(5):\n",
        "  visualize(\n",
        "      original_image = colour_code_segmentation(reverse_one_hot(xs[x]), rgb_vals),\n",
        "      ground_truth_mask = colour_code_segmentation(reverse_one_hot(ys[x]), rgb_vals),\n",
        "      #ground_truth_mask = colour_code_segmentation(reverse_one_hot(ys[x]), rgb_vals)\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfvplIy9RKOM"
      },
      "outputs": [],
      "source": [
        "X2 = torch.stack(xs)\n",
        "Y2 = torch.stack(ys)\n",
        "print(X2.shape)\n",
        "print(Y2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbpy4YadR2g0"
      },
      "outputs": [],
      "source": [
        "Y2 = Y2.float()\n",
        "X2 = X2.float()\n",
        "Y2.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqh8liXhGL6w"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.2, shuffle=True)\n",
        "\n",
        "print(x_train2.shape)\n",
        "print(y_train2.shape)\n",
        "\n",
        "\n",
        "#X = X.numpy().reindex(np.random.permutation(X.index))\n",
        "#one_hot_Y = one_hot_Y.numpy().reindex(np.random.permutation(one_hot_Y.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkBWjecNF6pv"
      },
      "outputs": [],
      "source": [
        "for x in range(10):\n",
        "  visualize(\n",
        "      original_image =  reverse_one_hot(x_train2[x]),\n",
        "      ground_truth_mask = reverse_one_hot(y_train2[x]),\n",
        "      #one_hot_encoded_mask = reverse_one_hot(y_test[x])\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04twRITjF6py"
      },
      "source": [
        "## create our datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2HCK0AnF6pz"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "import random\n",
        "\n",
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    \n",
        "    # store the augmented tensors tensors\n",
        "    self._x, self._y = x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    # a DataSet must know it size\n",
        "    return self._x.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.tensor(np.expand_dims(colour_code_segmentation(reverse_one_hot(torch.tensor(self._x[index, :])), rgb_vals),0).astype(float)).to(device=\"cuda\", dtype=torch.float)\n",
        "    y = self._y[index, :]\n",
        "    # print(\"GETTING ITEM\")\n",
        "    return x, y\n",
        "\n",
        "train_ds2 = MyDataSet(x_train2, y_train2)\n",
        "val_ds2 = MyDataSet(x_test2, y_test2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvz1wNuUF6p0"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXVQ-_zZF6p1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Get train and val data loaders\n",
        "train_loader2 = DataLoader(train_ds2, batch_size=64, shuffle=True)\n",
        "valid_loader2 = DataLoader(val_ds2, batch_size=20, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev6CGvfFF6p2"
      },
      "outputs": [],
      "source": [
        "print(len(train_loader2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W3L1BUgOgDF"
      },
      "outputs": [],
      "source": [
        "label_adapter = smp.Unet(\n",
        "    in_channels=1,\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=\"imagenet\", \n",
        "    classes=num_classes, \n",
        "    activation=ACTIVATION,\n",
        "    #encoder_depth = 18,\n",
        "    #decoder_channels = 18,\n",
        "    decoder_use_batchnorm = True,\n",
        "    #aux_params=aux_params\n",
        ")\n",
        "#preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffjcXhYYbHg5"
      },
      "outputs": [],
      "source": [
        "train_model(label_adapter, train_loader2, valid_loader2, \"/content/drive/MyDrive/FRESH/label_adapter.pth\", 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdYNLGNybHg6"
      },
      "source": [
        "### Load and view model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuWDEZ2QOP5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "import tensorflow as tf\n",
        "from torchmetrics.classification import F1Score, BinaryF1Score, MulticlassF1Score, JaccardIndex\n",
        "from torchmetrics import Dice\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "def average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "def view_label_predictions(model, ds, numm_classes ):\n",
        "\n",
        "    ious = []\n",
        "    f1s = []\n",
        "\n",
        "    #predict\n",
        "    for idx in range(len(ds)):\n",
        "\n",
        "        image, gt_mask = ds[idx]\n",
        "        image = image.cpu()\n",
        "        image_vis = image.cpu()\n",
        "        image_vis = np.transpose(image_vis,(1,2,0))\n",
        "        print(\"vis:\",image_vis.shape)\n",
        "        print(\"im:\",image.shape)\n",
        "        \n",
        "        x_tensor = torch.tensor(image).to(DEVICE).unsqueeze(0)\n",
        "        #print(\"X_TENSOR:\", x_tensor, x_tensor.shape)\n",
        "        # Predict test image\n",
        "        pred_mask = model(x_tensor)\n",
        "        print(\"predraw\", pred_mask.shape)\n",
        "        print(\"gtraw\", gt_mask.shape)\n",
        "        pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "        # Convert pred_mask from `CHW` format to `HWC` format\n",
        "        print(pred_mask.shape)\n",
        "        # Get prediction channel corresponding to face\n",
        "        pred_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(pred_mask)), rgb_vals)\n",
        "        print(pred_mask.shape)\n",
        "        \n",
        "        # Convert gt_mask from `CHW` format to `HWC` format\n",
        "        print(gt_mask.shape)\n",
        "        gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "        \n",
        "        #get IoU score\n",
        "        m = tf.keras.metrics.MeanIoU(num_classes=numm_classes)\n",
        "        m.update_state(gt_mask, pred_mask)\n",
        "        iou = m.result().numpy()\n",
        "        #print(\"MEAN IoU:\" , iou)\n",
        "        ious.append(iou)\n",
        "\n",
        "        #gt_mask = scipy.signal.medfilt(gt_mask, 9)\n",
        "\n",
        "        #Get f1\n",
        "        #m = MultiLabelBinarizer().fit(gt_mask)\n",
        "        #f1 = f1_score(m.transform(gt_mask), m.transform(pred_mask), average=None)\n",
        "        #if len(f1) == num_classes:\n",
        "        #  f1s.append(f1)\n",
        "\n",
        "        metric = MulticlassF1Score(num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        #metric = JaccardIndex(task=\"multiclass\",num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        f1 = metric(torch.tensor(pred_mask), torch.tensor(gt_mask))\n",
        "        if len(f1) == num_classes:\n",
        "          f1[f1 <0.1] = np.nan\n",
        "          print(f1)\n",
        "          f1s.append(np.array(f1))\n",
        "\n",
        "        try:\n",
        "          if idx < 20:\n",
        "            visualize(\n",
        "                original_image = image[0,::],\n",
        "                ground_truth_mask = gt_mask,\n",
        "                predicted_mask = pred_mask,\n",
        "            )\n",
        "        except:\n",
        "          if idx < 20:\n",
        "            visualize(\n",
        "                original_image = image.cuda()[0,::],\n",
        "                ground_truth_mask = gt_mask.cuda(),\n",
        "                predicted_mask = pred_mask.cuda(),\n",
        "            )\n",
        "\n",
        "    \n",
        "    fs1_numpy = np.array(f1s)\n",
        "    av_f1s = np.nanmean(fs1_numpy, axis=0)\n",
        "    #av_f1s = fs1_numpy.mean(axis=0)\n",
        "    av_f1s_av = av_f1s.mean(axis=0)\n",
        "\n",
        "    print (\"Dataset MIoU = \", average(ious))\n",
        "    print (\"Dataset F1 = \", av_f1s)\n",
        "    print (\"Dataset F1 av = \", av_f1s_av)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUpFShDlbHg6"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"/content/drive/MyDrive/FRESH/label_adapter.pth\", map_location=DEVICE)\n",
        "\n",
        "view_label_predictions(model,val_ds2, num_classes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "interpreter": {
      "hash": "01070a215043f07a8263edac27ed84a056fb8ad0ef2223506d9cf2bf6f4d5c3c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
