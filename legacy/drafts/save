# Training of Models with Helen*

> Taken from: "https://github.com/JPlin/Relabeled-HELEN-Dataset"

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
#!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip
#!tar xvzf "/content/drive/MyDrive/LaPa.tar.gz"
#!7z x "/content/drive/MyDrive/helenstar_release/2/helenstar_release.7z" -o"/content/drive/MyDrive/helenstar_release/2"
!wget   http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2 # DOWNLOAD LINK

!bunzip2 /content/shape_predictor_5_face_landmarks.dat.bz2

datFile =  "/content/shape_predictor_5_face_landmarks.dat"
import cv2
import glob
import torch
import numpy as np
import os
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from PIL import Image
import dlib
import torchvision
from torchvision import transforms


def shape_to_normal(shape):
  shape_normal = []
  for i in range(0, 5):
      shape_normal.append((i, (shape.part(i).x, shape.part(i).y)))
  return shape_normal

def get_eyes_nose_dlib(shape):
    nose = shape[4][1]
    left_eye_x = int(shape[3][1][0] + shape[2][1][0]) // 2
    left_eye_y = int(shape[3][1][1] + shape[2][1][1]) // 2
    right_eyes_x = int(shape[1][1][0] + shape[0][1][0]) // 2
    right_eyes_y = int(shape[1][1][1] + shape[0][1][1]) // 2
    return nose, (left_eye_x, left_eye_y), (right_eyes_x, right_eyes_y)

def distance(a, b):
    return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)

def cosine_formula(length_line1, length_line2, length_line3):
  cos_a = -(length_line3 ** 2 - length_line2 ** 2 - length_line1 ** 2) / (2 * length_line2 * length_line1)
  return cos_a

def rotate_point(origin, point, angle):
    ox, oy = origin
    px, py = point

    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)
    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)
    return qx, qy

def is_between(point1, point2, point3, extra_point):
    c1 = (point2[0] - point1[0]) * (extra_point[1] - point1[1]) - (point2[1] - point1[1]) * (extra_point[0] - point1[0])
    c2 = (point3[0] - point2[0]) * (extra_point[1] - point2[1]) - (point3[1] - point2[1]) * (extra_point[0] - point2[0])
    c3 = (point1[0] - point3[0]) * (extra_point[1] - point3[1]) - (point1[1] - point3[1]) * (extra_point[0] - point3[0])
    if (c1 < 0 and c2 < 0 and c3 < 0) or (c1 > 0 and c2 > 0 and c3 > 0):
        return True
    else:
        return False

def align(img):
    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)
    #face alignment
    rects = detector(gray, 0)
    if len(rects) > 0:
        for rect in rects:
            x = rect.left()
            y = rect.top()
            w = rect.right()
            h = rect.bottom()
            shape = predictor(gray, rect)
    else:
      return 0

    shape = shape_to_normal(shape)
    nose, left_eye, right_eye = get_eyes_nose_dlib(shape)

    center_of_forehead = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)

    center_pred = (int((x + w) / 2), int((y + y) / 2))

    length_line1 = distance(center_of_forehead, nose)
    length_line2 = distance(center_pred, nose)
    length_line3 = distance(center_pred, center_of_forehead)

    cos_a = cosine_formula(length_line1, length_line2, length_line3)
    angle = np.arccos(cos_a)

    rotated_point = rotate_point(nose, center_of_forehead, angle)
    rotated_point = (int(rotated_point[0]), int(rotated_point[1]))
    if is_between(nose, center_of_forehead, center_pred, rotated_point):
        angle = np.degrees(-angle)
    else:
        angle = np.degrees(angle)
    
    #gray = Image.fromarray(gray)
    #gray = np.array(gray.rotate(angle))

    return angle


#Resize images (height  = X, width = Y)
SIZE_X = 128 
SIZE_Y = 128
#Ensure MLP Is ony fed 1000/ Dataset
SAMPLE_SIZE = 1998

# crop face using haar cascade
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')

crops = []
original = 0
skips = []
rejected = []
angles = []

"""IMG"""
#Capture training image info as a list
train_images = []
val_images = []
for count, img_path in enumerate(sorted(glob.glob(os.path.join("/content/drive/MyDrive/helenstar_release/train/", "*image.jpg")))):
    #Process images
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    #cv2_imshow(img)
    #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    #train_images.append(img)
    
    # convert to gray
    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)

    # face bounding boxes
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor(datFile)  

    face = None

    # Detect faces
    try:
      face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_alt2.xml")
      limit = int((gray.shape[0]/2))
      faces = face_cascade.detectMultiScale(gray, 1.05, 2, minSize=[limit,0])
      face = sorted(faces,key=lambda f:f[2]*f[3])[-1]
    except:
      #print("FAILED USING FRONTAL FACE ALT 2")
      try:
        face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_profileface.xml")
        limit = int((gray.shape[0]/2))
        faces = face_cascade.detectMultiScale(gray, 1.05, 2, minSize=[limit,0])
        face = sorted(faces,key=lambda f:f[2]*f[3])[-1]
      except:
        try:
          face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
          limit_1 = int((gray.shape[0]/2.5))
          limit_2 = int((gray.shape[1]/1.7))
          faces = face_cascade.detectMultiScale(gray, 1.05, 3, minSize=[limit_1,0], maxSize=[50000,limit_2]) 
          face = sorted(faces,key=lambda f:f[2]*f[3])[-1]  
        except:
          #print("FAILED USING FRONTAL FACE ALT 2")
          try:
            face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
            limit_1 = int((gray.shape[0]/2.5))
            limit_2 = int((gray.shape[1]/2.5))
            faces = face_cascade.detectMultiScale(gray, minSize=[0,limit_2])
            face = sorted(faces,key=lambda f:f[2]*f[3])[-1]
          except:
            #print("FAILED USING FRONTAL FACE ALT 2")
            try:
              face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_alt.xml")
              limit_1 = int((gray.shape[0]/2))
              limit_2 = int((gray.shape[1]/3))
              faces = face_cascade.detectMultiScale(gray, 1.03, 1, minSize=[limit_1,0])
              face = sorted(faces,key=lambda f:f[2]*f[3])[-1]
            except:
              #print("FAILED USING PROFILE FACE ALT")
              print("->")
              skips.append(count)
              rejected.append(gray)
              crops.append([0,0,0,0])

    if face is not None:
      x, y, w, h = face
      crops.append(face)
      face = img[y:y + h, x:x + w]
      face = cv2.resize(face, (SIZE_Y, SIZE_X))
      #cv2_imshow(face)

      angle = align(img)
      face = Image.fromarray(face)
      face_rotated = np.array(face.rotate(angle))
      angles.append(angle)

      #print(count)
      #cv2_imshow(face_rotated)
      
      #print(normed_face.shape)

      train_images.append(face_rotated)
      #preview = torchvision.transforms.functional.to_pil_image(normed_face)
      #print(preview.shape)
      #plt.imshow(preview,cmap='gray', vmin=0, vmax=1)
      #plt.show()
      

    # Stop after SAMPLE_SIZE is reached
    if count % 9 == 0:
      print(str(count), "/", str(SAMPLE_SIZE)) #, end='\r'
    if count == SAMPLE_SIZE -1:
      break

X = torch.tensor(train_images).permute(0,3,1,2)
#X= torch.stack(train_images)
print(X.shape)

#for x in rejected:
#  cv2_imshow(x)
from scipy import signal

"""MASK"""
# Capture mask/label info as a list
masks = np.empty(shape=(int(len(train_images)),1,SIZE_Y,SIZE_Y))
print(len(crops))
print(len(train_images))
suc_count = 0
for count, mask_path in enumerate(sorted(glob.glob(os.path.join("/content/drive/MyDrive/helenstar_release/train/", "*label.png")))):
    # Process images
    mask = cv2.imread(mask_path, 0)

    # retrieve x,y,h,w
    face = crops[count]
    #print(face)

    #print(sum(face))

    if sum(face) != 0:
      x, y, w, h = face
      #print(face.size)
      face = mask[y:y + h, x:x + w]
      #print(face.size)
      #print(face.shape)
      mask = cv2.resize(face, (SIZE_Y, SIZE_X))

      mask = Image.fromarray(mask)
      mask = np.array(mask.rotate(angles[suc_count]))

      mask = scipy.ndimage.median_filter(mask, 5)


      #plt.imshow(mask, cmap='gray')
      #plt.show()
      # one hot encode
      
      #one_hot_Y = torch.nn.functional.one_hot(torch.tensor(mask).to(torch.int64), 11)
      #one_hot_Y = one_hot_Y.permute(2,0 , 1)


      # save
      masks[suc_count]= (mask)
      suc_count +=1
    else:
      pass


    """END OF Y"""
    # Stop after SAMPLE_SIZE is reached
    print(str(count), "/", str(SAMPLE_SIZE), end='\r')
    if count == SAMPLE_SIZE -1:
      break

Y = torch.from_numpy(masks)

print(Y.shape)
print(original)
print(Y.shape)
print(X.shape)
#rgb_vals = np.unique(ar = Y.numpy())
rgb_vals = [ 0,1,2,3,4,5,6,7,8,9,10]
print(rgb_vals)

num_classes=len(rgb_vals)
import torchvision
import torchvision
import random

def gauss_noise_tensor(img):
    assert isinstance(img, torch.Tensor)
    dtype = img.dtype
    if not img.is_floating_point():
        img = img.to(torch.float32)
    
    sigma = 25.0
    
    out = img + sigma * torch.randn_like(img)
    
    if out.dtype != dtype:
        out = out.to(dtype)
        
    return out

def augment(x,y):
    if 0 == 0:
      # rotate XY
      prob_rot = random.randint(0,10)
      if prob_rot > 8:
        r = random.randint(-18,18)
        x = torchvision.transforms.functional.rotate(x, r)
        y = torchvision.transforms.functional.rotate(y, r)

      # persepective warp XY with percentage: 40% WARP
      prob_warp = random.randint(0,10)
      starting_coords = [[0,SIZE_Y],[SIZE_X,SIZE_Y], [0,0], [SIZE_X,0]]
      end_coords = starting_coords
      if prob_warp > 8:
        
        for coord in end_coords:
          for val in coord:
            transformation = random.random()
            val = val * transformation

        x, y = torchvision.transforms.functional.perspective(x, starting_coords, end_coords), torchvision.transforms.functional.perspective(y, starting_coords, end_coords)

      # (Gaussian) blurs XY with percentage: 30% BLUR
      #prob_blur = random.randint(0,10)
      #if prob_blur > 8:
      #  blurrer = torchvision.transforms.GaussianBlur(kernel_size=(3,5))
      #  x, y = blurrer(x), blurrer(y)

      # (Gaussian) noise XY with percentage: 30% noise
      #prob_noise = random.randint(0,10)
      #if prob_noise > 8:
      #  x = gauss_noise_tensor(x)

      #modulations to brightness and contrast X with percentage: 30% MODULATION
      prob_mod = random.randint(0,10)
      if prob_mod > 8:
        jitter = torchvision.transforms.ColorJitter()
        x = jitter(x)

      #print("auged")

      #prob_mod = random.randint(0,10)
      #if prob_mod > 0.7:
      #  x = F.hflip(x)
      #  y = F.hflip(y)

    #conversion to grayscale X
    #x = T.Grayscale()(x)
    
    return x,y
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
indices = [23, 43, 52, 79, 80, 122, 136, 210, 259, 23] # Arbitrary list of indices that should be removed

indices_to_keep = [i for i in range(x_test.shape[0]) if i not in indices] # List of indices that should be kept
x_test = x_test[torch.LongTensor(indices_to_keep)]

indices_to_keep = [i for i in range(y_test.shape[0]) if i not in indices] # List of indices that should be kept
y_test = y_test[torch.LongTensor(indices_to_keep)]

print(x_test.shape)
print(y_test.shape)
indices = [1458, 828, 941, 1198, 1301, 1390, 778, 203, 241, 354, 564] # Arbitrary list of indices that should be removed

indices_to_keep = [i for i in range(x_train.shape[0]) if i not in indices] # List of indices that should be kept
x_train = x_train[torch.LongTensor(indices_to_keep)]

indices_to_keep = [i for i in range(y_train.shape[0]) if i not in indices] # List of indices that should be kept
y_train = y_train[torch.LongTensor(indices_to_keep)]

print(x_train.shape)
print(y_train.shape)