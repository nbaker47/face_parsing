{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OaZQL5Lk1Fo"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights, lraspp_mobilenet_v3_large, deeplabv3_resnet101\n",
        "\n",
        "#Hyperparamters\n",
        "ENCODER = 'resnet101'\n",
        "ENCODER_WEIGHTS = 'imagenet' #pretrained weighting\n",
        "#CLASSES = [\"background\", \"skin\", \"nose\", \"right_eye\", \"left_eye\", \"right_brow\", \"left_brow\", \"right_ear\", \"left_ear\", \"mouth_interior\", \"top_lip\", \"bottom_lip\", \"neck\", \"hair\", \"beard\", \"clothing\", \"glasses\", \"headwear\", \"facewear\"]\n",
        "ACTIVATION = \"sigmoid\" # softmax2d for multiclass segmentation\n",
        "num_classes = 12\n",
        "\n",
        "\"\"\"\n",
        "aux_params=dict(\n",
        "    pooling='avg',       # one of 'avg', 'max'\n",
        "    classes=num_classes, \n",
        "    activation = None\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "#DEEPLABV3+\n",
        "\n",
        "deeplab = smp.DeepLabV3Plus(\n",
        "    in_channels=3,\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=num_classes, \n",
        "    activation=ACTIVATION,\n",
        "    #encoder_depth = 18,\n",
        "    #decoder_channels = 18,\n",
        "    #aux_params=aux_params\n",
        ")\n",
        "\n",
        "\n",
        "#UNET\n",
        "unet = smp.Unet(\n",
        "    in_channels=3,\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=num_classes, \n",
        "    activation=ACTIVATION,\n",
        "    #encoder_depth = 18,\n",
        "    #decoder_channels = 18,\n",
        "    decoder_use_batchnorm = True,\n",
        "    #aux_params=aux_params\n",
        ")\n",
        "\n",
        "fcn = smp.FPN(\n",
        "    in_channels=3,\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=num_classes, \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "mobile = smp.Unet(\n",
        "    in_channels=3,\n",
        "    encoder_name=\"mobilenet_v2\", \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=num_classes, \n",
        "    activation=ACTIVATION,\n",
        "    decoder_use_batchnorm = True,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1jVz-ceu0LC"
      },
      "outputs": [],
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row (Original Image: Ground Truth : Predicted)\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        if name == \"original_image\":\n",
        "          #print(image.shape)\n",
        "          plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "          plt.imshow(image)\n",
        "        else:\n",
        "          #print(image.shape)\n",
        "          # get title from the parameter names\n",
        "          plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "          plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "#Function to reverse one-hot-encode an image\n",
        "def reverse_one_hot(image):\n",
        "    #argmax to return the indices of the maximum values along an axis. \n",
        "    x = np.argmax(image, axis = -3)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    colour_codes = np.array(label_values)\n",
        "    print(colour_codes)\n",
        "    \"\"\"\n",
        "    mask = np.array(image)\n",
        "    #convert mask to helen labelw\n",
        "    mask[mask == 0] = 100 #back\n",
        "    mask[mask == 1] = 101 #skin\n",
        "    mask[mask == 2] = 102 #nose\n",
        "    mask[mask == 3] = 103 #right eye\n",
        "    mask[mask == 4] = 104 #left eye\n",
        "    mask[mask == 5] = 105 #right brow\n",
        "    mask[mask == 6] = 106 # left brow\n",
        "    mask[mask == 7] = 107 # right ear\n",
        "    mask[mask == 8] = 108# left ear\n",
        "    mask[mask == 9] = 109 # inner mouth\n",
        "    mask[mask == 10] = 110# top lip\n",
        "    mask[mask == 11] = 111# bottom lip\n",
        "    mask[mask == 12] = 112# neck\n",
        "    mask[mask == 13] = 113# hair\n",
        "    mask[mask == 14] = 114# beard\n",
        "    mask[mask == 15] = 115#clothing\n",
        "\n",
        "    mask[mask == 100] = 0 #back\n",
        "    mask[mask == 101] = 1 #skin\n",
        "    mask[mask == 102] = 6 #nose\n",
        "    mask[mask == 103] = 5 #right eye\n",
        "    mask[mask == 104] = 4 #left eye\n",
        "    mask[mask == 105] = 3 #right brow\n",
        "    mask[mask == 106] = 2 # left brow\n",
        "    mask[mask == 107] = 1 # right ear\n",
        "    mask[mask == 108] = 1# left ear\n",
        "    mask[mask == 109] = 8 # inner mouth\n",
        "    mask[mask == 110] = 7# top lip\n",
        "    mask[mask == 111] = 9# bottom lip\n",
        "    mask[mask == 112] = 1# neck\n",
        "    mask[mask == 113] = 10# hair\n",
        "    mask[mask >10] = 11# beard\n",
        "    print(np.unique(image))\n",
        "    \"\"\"\n",
        "    x = colour_codes[image.numpy().astype(int)]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A95DA-azk1Fl",
        "outputId": "cfaeb956-aaaf-4490-b9fc-2b2f261c916f"
      },
      "outputs": [],
      "source": [
        "#import torchvision.transforms as T\n",
        "#import torchvision.transforms.functional as F\n",
        "import albumentations as albu\n",
        "import random\n",
        "import scipy\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "#from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "\n",
        "SIZEX = 128\n",
        "SIZEY = 128\n",
        "\n",
        "rgb_vals = [ 0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    x_t = x.transpose(2, 0, 1).astype('float32')\n",
        "    #print(\"XTSHAPE\", x_t.shape)\n",
        "    return x_t\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        #albu.HorizontalFlip(p=0.5),\n",
        "        albu.Rotate((-18,18)),\n",
        "        albu.PadIfNeeded(min_height=SIZEY, min_width=SIZEY, always_apply=True, border_mode=0),\n",
        "        #albu.RandomCrop(height=320, width=320, always_apply=True),\n",
        "        albu.Perspective(p=0.5),\n",
        "        albu.GaussNoise(p=0.2),\n",
        "        albu.OneOf([albu.CLAHE(p=1),albu.RandomBrightness(p=1),albu.RandomGamma(p=1),],p=0.9,),\n",
        "        albu.OneOf([albu.Sharpen(p=1),albu.Blur(blur_limit=3, p=1),],p=0.9,),albu.OneOf([albu.RandomContrast(p=1),albu.HueSaturationValue(p=1),],p=0.9,),\n",
        "    ]\n",
        "\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def transformation_augs():\n",
        "    train_transform = [\n",
        "        #albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(SIZEY, SIZEY)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "\n",
        "  #CLASSES =  [\"background\",\"facial_skin\",\"left_brow\",\"right_brow\",\"left_eye\",\"right_eye\", \"nose\",\"upper_lip\",\"inner_mouth\",\"lower_lip\",\"hair\"]\n",
        "\n",
        "  def __init__(self, images_dir, masks_dir, coords_dir, preprocessing=None, classes=None,augmentation=None, mode=\"train\"):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    \n",
        "    # store the augmented tensors tensors\n",
        "    #self._x, self._y = x,y\n",
        "    self.preprocessing = preprocessing\n",
        "    self.augmentation = augmentation\n",
        "\n",
        "    self.image_ids = os.listdir(images_dir)\n",
        "\n",
        "    if mode == \"train\":\n",
        "      self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.image_ids][200::]\n",
        "      self.masks_fps = [os.path.join(masks_dir, mask_id.replace(\"_seg\",\"\")) for mask_id in self.image_ids][200::]\n",
        "      self.coords_fps = [os.path.join(coords_dir, coords_id.replace(\"_seg.png\", \"_ldmks.txt\")) for coords_id in self.image_ids][200::]\n",
        "    else:\n",
        "      self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.image_ids][:200]\n",
        "      self.masks_fps = [os.path.join(masks_dir, mask_id.replace(\"_seg\",\"\")) for mask_id in self.image_ids][:200]\n",
        "      self.coords_fps = [os.path.join(coords_dir, coords_id.replace(\"_seg.png\", \"_ldmks.txt\")) for coords_id in self.image_ids][:200]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    # a DataSet must know it size\n",
        "    return len(self.images_fps)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "\n",
        "    image = cv2.imread(self.masks_fps[i])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.imread(self.images_fps[i],0)\n",
        "\n",
        "    # crop to center face\n",
        "    smallest_x = 99999\n",
        "    smallest_y = 99999\n",
        "    biggest_x = -99999\n",
        "    biggest_y = -99999\n",
        "    with open(self.coords_fps[i], 'rb') as f:\n",
        "\n",
        "      contents = str(f.read()).split(\"\\\\n\")\n",
        "      try:\n",
        "        contents = [[int(float(single.replace(\"\\\\r\", \"\").replace(\"'\", \"\").replace(\"b\", \"\").replace(\"\\\\x1a\",\"\"))) for single in pair.split(\" \")] for pair in contents[1:-1]]\n",
        "      except:\n",
        "        print(contents)\n",
        "\n",
        "      #contents = np.array(contents)\n",
        "      for pair in contents:\n",
        "        #plt.scatter((pair[0]), (pair[1]), color=\"red\")\n",
        "\n",
        "        if (pair[0]) < smallest_x:\n",
        "          smallest_x = (pair[0])\n",
        "        \n",
        "        if (pair[0]) > biggest_x:\n",
        "          biggest_x = (pair[0])\n",
        "\n",
        "        if (pair[1]) < smallest_y:\n",
        "          smallest_y = (pair[1])\n",
        "        \n",
        "        if (pair[1]) > biggest_y:\n",
        "          biggest_y = (pair[1])\n",
        "\n",
        "\n",
        "      crop_coords = (int(smallest_x), int(biggest_x), int(smallest_y), int(biggest_y))\n",
        "\n",
        "      image = image[smallest_y: biggest_y, smallest_x:biggest_x]\n",
        "      mask = mask[smallest_y: biggest_y, smallest_x:biggest_x]\n",
        "\n",
        "    mask = cv2.resize(mask, (SIZEY, SIZEY))\n",
        "    image = cv2.resize(image, (SIZEY, SIZEY))\n",
        "    mask = np.expand_dims(mask,2)\n",
        "\n",
        "    #convert mask to helen labelw\n",
        "    mask[mask == 0] = 100 #back\n",
        "    mask[mask == 1] = 101 #skin\n",
        "    mask[mask == 2] = 102 #nose\n",
        "    mask[mask == 3] = 103 #right eye\n",
        "    mask[mask == 4] = 104 #left eye\n",
        "    mask[mask == 5] = 105 #right brow\n",
        "    mask[mask == 6] = 106 # left brow\n",
        "    mask[mask == 7] = 107 # right ear\n",
        "    mask[mask == 8] = 108# left ear\n",
        "    mask[mask == 9] = 109 # inner mouth\n",
        "    mask[mask == 10] = 110# top lip\n",
        "    mask[mask == 11] = 111# bottom lip\n",
        "    mask[mask == 12] = 112# neck\n",
        "    mask[mask == 13] = 113# hair\n",
        "    mask[mask == 14] = 114# beard\n",
        "    mask[mask == 15] = 115#clothing\n",
        "\n",
        "    mask[mask == 100] = 0 #back\n",
        "    mask[mask == 101] = 1 #skin\n",
        "    mask[mask == 102] = 6 #nose\n",
        "    mask[mask == 103] = 5 #right eye\n",
        "    mask[mask == 104] = 4 #left eye\n",
        "    mask[mask == 105] = 3 #right brow\n",
        "    mask[mask == 106] = 2 # left brow\n",
        "    mask[mask == 107] = 1 # right ear\n",
        "    mask[mask == 108] = 1# left ear\n",
        "    mask[mask == 109] = 8 # inner mouth\n",
        "    mask[mask == 110] = 7# top lip\n",
        "    mask[mask == 111] = 9# bottom lip\n",
        "    mask[mask == 112] = 1# neck\n",
        "    mask[mask == 113] = 10# hair\n",
        "    mask[mask >10] = 11# ignore\n",
        "\n",
        "    #print(np.unique(mask))\n",
        "\n",
        "    # smooth mask\n",
        "    # mask = mask = scipy.ndimage.median_filter(mask, 5)\n",
        "\n",
        "    # apply augmentations\n",
        "    if self.augmentation:\n",
        "        sample = self.augmentation(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "    \n",
        "    # apply preprocessing\n",
        "    if self.preprocessing:\n",
        "        sample = self.preprocessing(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "\n",
        "\n",
        "    #onehot\n",
        "    one_hot_Y = torch.nn.functional.one_hot(torch.tensor(mask).to(torch.int64), 12).permute(0,3,1,2).float()\n",
        "\n",
        "    return (image, one_hot_Y.squeeze(0))\n",
        "\n",
        "\n",
        "train_x_path = \"/home/nathan/Documents/final_project/datasets/wood/images\"\n",
        "train_y_path = \"/home/nathan/Documents/final_project/datasets/wood/labels\"\n",
        "train_coords_path = \"/home/nathan/Documents/final_project/datasets/wood/landmarks\"\n",
        "\n",
        "train_ds = MyDataSet(train_y_path, train_x_path, train_coords_path, preprocessing=get_preprocessing(preprocessing_fn),augmentation=get_training_augmentation(), mode=\"train\")\n",
        "val_ds = MyDataSet(train_y_path, train_x_path, train_coords_path, preprocessing=get_preprocessing(preprocessing_fn),augmentation=get_validation_augmentation(), mode=\"val\")\n",
        "image_vis, gt_mask = train_ds[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ug8TWvk0ntQ",
        "outputId": "e1b9f262-dc11-44ff-da0a-aea121a204bd"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "print(len(train_ds))\n",
        "\n",
        "for x in range(2):\n",
        "\n",
        "  image_vis, gt_mask = train_ds[50]\n",
        "  print(x, \":\", image_vis.shape, gt_mask.shape)\n",
        "\n",
        "  gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "  #binary_smoothed = scipy.signal.medfilt (gt_mask, 9)\n",
        "\n",
        "  im = Image.fromarray(np.uint8(gt_mask))\n",
        "  im.save(\"test.png\")\n",
        "\n",
        "\n",
        "  visualize(\n",
        "      original_image = image_vis[0,::],\n",
        "      ground_truth_mask = gt_mask\n",
        "  )\n",
        "\n",
        "#np.unique(image_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crDFzVbzk1Fm"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DHpDVR-k1Fm",
        "outputId": "c328ff70-9e13-4303-f832-54c5d49fa3e8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Get train and val data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=24, shuffle=True, num_workers=7)\n",
        "valid_loader = DataLoader(val_ds, batch_size=24, shuffle=False, num_workers=7)\n",
        "\n",
        "print(len(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT6IczkXZ3Mq"
      },
      "source": [
        "### (DataLoader) Sanity check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKwG2RgVk1Fn"
      },
      "source": [
        "## Initialise Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXUFuze5k1Fo"
      },
      "source": [
        "[link text](https:// [link text](https://))### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK2KirVsqHMn",
        "outputId": "4729aae6-fc2e-4635-e192-1b69a5336436"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "from sklearn import preprocessing\n",
        "\n",
        "weights_per_image = []\n",
        "for x, pair in enumerate(train_ds):\n",
        "  mask = pair[1]\n",
        "  weights = []\n",
        "  for layer in np.array(mask):\n",
        "    count = np.sum(layer == 1)\n",
        "    if count != 0:\n",
        "      weights.append(count)\n",
        "    else:\n",
        "      weights.append(0)\n",
        "  if x == 100:\n",
        "    break\n",
        "\n",
        "  weights_per_image.append(weights)\n",
        "\n",
        "# average down collumns\n",
        "wpi_numpy = np.array(weights_per_image)\n",
        "wpi_avs = wpi_numpy.mean(axis=0)\n",
        "\n",
        "normedWeights = [1 - (x / sum(wpi_avs)) for x in wpi_avs]\n",
        "normedWeights = torch.FloatTensor(normedWeights)\n",
        "normedWeights[-1] = 0.0 #ignore index\n",
        "normedWeights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXGkPrD3k1Fo"
      },
      "outputs": [],
      "source": [
        "from segmentation_models_pytorch import utils\n",
        "import os\n",
        "\n",
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "TRAINING = True\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "#\"cuda\" if torch.cuda.is_available() else\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#loss = smp.utils.losses.BCELoss()\n",
        "#loss = FocalLoss(\"multilabel\", alpha=0.25, gamma=4)\n",
        "#weights = torch.tensor([0.7526, 0.4953, 0.9898, 0.9910, 0.9942, 0.9942, 0.9579, 0.9940, 0.9931,\n",
        "#        0.9895, 0.8484])\n",
        "\n",
        "#loss = SoftBCEWithLogitsLoss(pos_weight=normedWeights)\n",
        "loss = smp.utils.losses.CrossEntropyLoss(weight=normedWeights)\n",
        "\n",
        "# define evalusation metrics\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqWwaSlFk1Fp"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJd8tN1_k1Fp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, train_loader_given, valid_loader_given, saveas, epoch):\n",
        "\n",
        "    #clear CUDA cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # define optimizer\n",
        "    #optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.001)])\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
        "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # define learning rate scheduler\n",
        "    #lr_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer)\n",
        "\n",
        "    #define train ecpochs w/ our hyperparams\n",
        "    train_epoch = smp.utils.train.TrainEpoch(\n",
        "        model, \n",
        "        loss=loss, \n",
        "        metrics=metrics, \n",
        "        optimizer=optimizer,\n",
        "        device=DEVICE,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    #define train ecpochs w/ our hyperparams\n",
        "    valid_epoch = smp.utils.train.ValidEpoch(\n",
        "        model, \n",
        "        loss=loss, \n",
        "        metrics=metrics, \n",
        "        device=DEVICE,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    if TRAINING:\n",
        "\n",
        "        best_iou_score = 0.0\n",
        "        train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "        #Keep track of epoch BCE and IoU for graph plots\n",
        "        epoch_count = []\n",
        "        v_bce = []\n",
        "        v_ious = []\n",
        "\n",
        "        t_bce = []\n",
        "        t_ious = []\n",
        "\n",
        "        i = 0\n",
        "        while i < epoch:\n",
        "            i +=1 \n",
        "\n",
        "            # Perform training & validation\n",
        "            print('\\nEpoch: {}'.format(i))\n",
        "            train_logs = train_epoch.run(train_loader_given)\n",
        "            valid_logs = valid_epoch.run(valid_loader_given)\n",
        "            #lr_scheduler.step()\n",
        "\n",
        "            #log\n",
        "            train_logs_list.append(train_logs)\n",
        "            valid_logs_list.append(valid_logs)\n",
        "\n",
        "            # Save model if a better val IoU score is obtained\n",
        "            if best_iou_score < valid_logs['iou_score']:\n",
        "                best_iou_score = valid_logs['iou_score']\n",
        "                torch.save(model, saveas)\n",
        "                print('Model saved!')\n",
        "\n",
        "            v_ious.append(valid_logs['iou_score'])\n",
        "            t_ious.append(train_logs['iou_score'])\n",
        "            \n",
        "            epoch_count.append(i)\n",
        "        \n",
        "            # Plot graph every 2 epochs\n",
        "            if i % 2 ==0:\n",
        "                plt.show()\n",
        "                plt.plot(epoch_count,t_ious, label=\"Train IoU\")\n",
        "                plt.plot(epoch_count,v_ious, label=\"Valid IoU\")\n",
        "                plt.legend(loc=\"upper left\")\n",
        "                plt.xlabel(\"Epochs\")\n",
        "                plt.ylabel(\"IoU\")\n",
        "                plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuxQc7MKQJy0"
      },
      "outputs": [],
      "source": [
        "#! pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3WowjSXk1Fq"
      },
      "source": [
        "### Function to view train model precitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buTwNEGak1Fq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "import tensorflow as tf\n",
        "from torchmetrics.classification import F1Score, BinaryF1Score, MulticlassF1Score, JaccardIndex\n",
        "from torchmetrics import Dice\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "def average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "def view_predictions(model, ds, numm_classes ):\n",
        "\n",
        "    ious = []\n",
        "    f1s = []\n",
        "\n",
        "    #predict\n",
        "    for idx in range(10):\n",
        "\n",
        "        image, gt_mask = ds[idx]\n",
        "        image = image\n",
        "        image_vis = image\n",
        "        image_vis = np.transpose(image_vis,(1,2,0))\n",
        "        #print(\"vis:\",image_vis.shape)\n",
        "        #print(\"im:\",image.shape)\n",
        "\n",
        "        \n",
        "        \n",
        "        x_tensor = torch.tensor(image).to(DEVICE).unsqueeze(0)\n",
        "        #print(\"X_TENSOR:\", x_tensor, x_tensor.shape)\n",
        "        # Predict test image\n",
        "        pred_mask = model(x_tensor)\n",
        "\n",
        "        #print(\"predraw\", pred_mask.shape)\n",
        "        #print(\"gtraw\", gt_mask.shape)\n",
        "        pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "        # Convert pred_mask from `CHW` format to `HWC` format\n",
        "        #print(pred_mask.shape)\n",
        "        # Get prediction channel corresponding to face\n",
        "        pred_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(pred_mask)), rgb_vals)\n",
        "        #print(pred_mask.shape)\n",
        "        \n",
        "        # Convert gt_mask from `CHW` format to `HWC` format\n",
        "        #print(gt_mask.shape)\n",
        "        gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "        \n",
        "        #get IoU score\n",
        "        m = tf.keras.metrics.MeanIoU(num_classes=numm_classes)\n",
        "        m.update_state(gt_mask, pred_mask)\n",
        "        iou = m.result().numpy()\n",
        "        #print(\"MEAN IoU:\" , iou)\n",
        "        ious.append(iou)\n",
        "\n",
        "        metric = MulticlassF1Score(num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        #metric = JaccardIndex(task=\"multiclass\",num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        f1 = metric(torch.tensor(gt_mask), torch.tensor(pred_mask))\n",
        "        if len(f1) == num_classes:\n",
        "          f1[f1 <0.1] = np.nan\n",
        "          #print(f1)\n",
        "          f1s.append(np.array(f1))\n",
        "\n",
        "        if idx:\n",
        "          visualize(\n",
        "              original_image = image[0,::],\n",
        "              ground_truth_mask = gt_mask,\n",
        "              predicted_mask = pred_mask,\n",
        "          )\n",
        "\n",
        "    \n",
        "    fs1_numpy = np.array(f1s)\n",
        "    av_f1s = np.nanmean(fs1_numpy, axis=0)\n",
        "    #av_f1s = fs1_numpy.mean(axis=0)\n",
        "    av_f1s_av = av_f1s.mean(axis=0)\n",
        "\n",
        "    print (\"Dataset MIoU = \", average(ious))\n",
        "    print (\"Dataset F1 = \", av_f1s)\n",
        "    print (\"Dataset F1 av = \", av_f1s_av)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUjn7DVPBcr0"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "def save_predictions(model, ds):\n",
        "\n",
        "  xs = []\n",
        "  ys = []\n",
        "\n",
        "  #predict\n",
        "  for idx in range(len(ds)):\n",
        "\n",
        "      # Pop image from DS\n",
        "      image, gt_mask = ds[idx]\n",
        "      image_vis = image\n",
        "      image_vis = np.transpose(image_vis,(1,2,0))\n",
        "      \n",
        "      # Reshape\n",
        "      x_tensor = torch.tensor(image).to(DEVICE).unsqueeze(0)\n",
        "      # Predict test image\n",
        "      pred_mask = model(x_tensor)\n",
        "      # Reshape\n",
        "      pred_mask = pred_mask.detach().squeeze().cpu()\n",
        "      \n",
        "      # Decode Onehots\n",
        "      #pred_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(pred_mask)), rgb_vals)\n",
        "      #gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "\n",
        "      # Sanity check\n",
        "    #  print(image_vis.shape)\n",
        "      #print(\"GT\", gt_mask.shape)\n",
        "     # print(\"PRED\", pred_mask.shape)\n",
        "\n",
        "      #gt_mask = scipy.signal.medfilt(gt_mask, 9)\n",
        "\n",
        "      print(\"saving\", idx, \"/\", len(ds))\n",
        "\n",
        "      # Save Predictions for use in Label Adapter\n",
        "      xs.append(pred_mask)\n",
        "\n",
        "      ys.append(gt_mask)\n",
        "      \n",
        "  return xs, ys\n",
        "\n",
        "#save_predictions(model, val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD5uZhW8k1Fq"
      },
      "source": [
        "# Predict with Deeplabv3+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aDw3mbsrk1Fr",
        "outputId": "0e0b7a3e-8a11-45b8-8223-bcd9ed477fe4"
      },
      "outputs": [],
      "source": [
        "train_model(unet, train_loader, valid_loader, \"/home/nathan/Documents/final_project/saved_models/synth_100000/unet.pth\", 50)\n",
        "train_model(fcn, train_loader, valid_loader, \"/home/nathan/Documents/final_project/saved_models/synth_100000/fcn.pth\", 50)\n",
        "train_model(mobile, train_loader, valid_loader, \"/home/nathan/Documents/final_project/saved_models/synth_100000/mobile.pth\", 50)\n",
        "train_model(deeplab, train_loader, valid_loader, \"/home/nathan/Documents/final_project/saved_models/synth_100000/deeplab.pth\", 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTKVQT5ecjtP"
      },
      "source": [
        "### Load and view model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VycUDUz-VRaF"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"/home/nathan/Documents/final_project/saved_models/synth/unet.pth\", map_location=DEVICE)\n",
        "view_predictions(model,train_ds, num_classes)\n",
        "\n",
        "model = torch.load(\"/content/drive/MyDrive/FRESH/helen/fcn.pth\", map_location=DEVICE)\n",
        "view_predictions(model,train_ds, num_classes)\n",
        "\n",
        "model = torch.load(\"/content/drive/MyDrive/FRESH/helen/mobile.pth\", map_location=DEVICE)\n",
        "view_predictions(model,train_ds, num_classes)\n",
        "\n",
        "model = torch.load(\"/content/drive/MyDrive/FRESH/helen/deeplab.pth\", map_location=DEVICE)\n",
        "view_predictions(model,train_ds, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ZyVrbFB_lI"
      },
      "source": [
        "# label adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy9r-A2L2JmF",
        "outputId": "c3f4302b-5f7b-46ee-d36e-42361a7052e4"
      },
      "outputs": [],
      "source": [
        "xs, ys = save_predictions(model, train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RBSurf5YymN",
        "outputId": "28d36137-6288-4430-98fe-70473617fd7d"
      },
      "outputs": [],
      "source": [
        "len(xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UYbvP4uHXaa5",
        "outputId": "f6164dc6-cfff-4cfc-e6f2-05214c993e7b"
      },
      "outputs": [],
      "source": [
        "for x in range(5):\n",
        "  visualize(\n",
        "      original_image = colour_code_segmentation(reverse_one_hot(xs[x]), rgb_vals),\n",
        "      ground_truth_mask = colour_code_segmentation(reverse_one_hot(ys[x]), rgb_vals),\n",
        "      #ground_truth_mask = colour_code_segmentation(reverse_one_hot(ys[x]), rgb_vals)\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfvplIy9RKOM",
        "outputId": "5ee17585-8266-4de9-c463-56adde994a7e"
      },
      "outputs": [],
      "source": [
        "X2 = torch.stack(xs)\n",
        "Y2 = torch.stack(ys)\n",
        "print(X2.shape)\n",
        "print(Y2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbpy4YadR2g0",
        "outputId": "15d55531-3235-453a-98ad-a83bac6dda60"
      },
      "outputs": [],
      "source": [
        "Y2 = Y2.float()\n",
        "X2 = X2.float()\n",
        "Y2.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqh8liXhGL6w",
        "outputId": "6dc8d627-ae4d-4e56-fa0b-1727a54db06f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.2, shuffle=True)\n",
        "\n",
        "print(x_train2.shape)\n",
        "print(y_train2.shape)\n",
        "\n",
        "\n",
        "#X = X.numpy().reindex(np.random.permutation(X.index))\n",
        "#one_hot_Y = one_hot_Y.numpy().reindex(np.random.permutation(one_hot_Y.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jkBWjecNF6pv",
        "outputId": "10ae51d6-26dc-4e7b-b148-b8fdaefa4c50"
      },
      "outputs": [],
      "source": [
        "for x in range(10):\n",
        "  visualize(\n",
        "      original_image =  reverse_one_hot(x_train2[x]),\n",
        "      ground_truth_mask = reverse_one_hot(y_train2[x]),\n",
        "      #one_hot_encoded_mask = reverse_one_hot(y_test[x])\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04twRITjF6py"
      },
      "source": [
        "## create our datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2HCK0AnF6pz"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "import random\n",
        "\n",
        "class MyDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    \n",
        "    # store the augmented tensors tensors\n",
        "    self._x, self._y = x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    # a DataSet must know it size\n",
        "    return self._x.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.tensor(np.expand_dims(colour_code_segmentation(reverse_one_hot(torch.tensor(self._x[index, :])), rgb_vals),0).astype(float)).to(device=\"cuda\", dtype=torch.float)\n",
        "    y = self._y[index, :]\n",
        "    # print(\"GETTING ITEM\")\n",
        "    return x, y\n",
        "\n",
        "train_ds2 = MyDataSet(x_train2, y_train2)\n",
        "val_ds2 = MyDataSet(x_test2, y_test2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvz1wNuUF6p0"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXVQ-_zZF6p1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Get train and val data loaders\n",
        "train_loader2 = DataLoader(train_ds2, batch_size=64, shuffle=True)\n",
        "valid_loader2 = DataLoader(val_ds2, batch_size=20, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev6CGvfFF6p2",
        "outputId": "10b03abf-e2b8-4a2d-c563-bc44446ff416"
      },
      "outputs": [],
      "source": [
        "print(len(train_loader2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W3L1BUgOgDF"
      },
      "outputs": [],
      "source": [
        "label_adapter = smp.Unet(\n",
        "    in_channels=1,\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=\"imagenet\", \n",
        "    classes=num_classes, \n",
        "    activation=ACTIVATION,\n",
        "    #encoder_depth = 18,\n",
        "    #decoder_channels = 18,\n",
        "    decoder_use_batchnorm = True,\n",
        "    #aux_params=aux_params\n",
        ")\n",
        "#preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ffjcXhYYbHg5",
        "outputId": "a41b50fe-d489-418d-bfe0-8282b932d529"
      },
      "outputs": [],
      "source": [
        "train_model(label_adapter, train_loader2, valid_loader2, \"/content/drive/MyDrive/FRESH/label_adapter.pth\", 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdYNLGNybHg6"
      },
      "source": [
        "### Load and view model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuWDEZ2QOP5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "import tensorflow as tf\n",
        "from torchmetrics.classification import F1Score, BinaryF1Score, MulticlassF1Score, JaccardIndex\n",
        "from torchmetrics import Dice\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "def average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "def view_label_predictions(model, ds, numm_classes ):\n",
        "\n",
        "    ious = []\n",
        "    f1s = []\n",
        "\n",
        "    #predict\n",
        "    for idx in range(len(ds)):\n",
        "\n",
        "        image, gt_mask = ds[idx]\n",
        "        image = image.cpu()\n",
        "        image_vis = image.cpu()\n",
        "        image_vis = np.transpose(image_vis,(1,2,0))\n",
        "        print(\"vis:\",image_vis.shape)\n",
        "        print(\"im:\",image.shape)\n",
        "        \n",
        "        x_tensor = torch.tensor(image).to(DEVICE).unsqueeze(0)\n",
        "        #print(\"X_TENSOR:\", x_tensor, x_tensor.shape)\n",
        "        # Predict test image\n",
        "        pred_mask = model(x_tensor)\n",
        "        print(\"predraw\", pred_mask.shape)\n",
        "        print(\"gtraw\", gt_mask.shape)\n",
        "        pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "        # Convert pred_mask from `CHW` format to `HWC` format\n",
        "        print(pred_mask.shape)\n",
        "        # Get prediction channel corresponding to face\n",
        "        pred_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(pred_mask)), rgb_vals)\n",
        "        print(pred_mask.shape)\n",
        "        \n",
        "        # Convert gt_mask from `CHW` format to `HWC` format\n",
        "        print(gt_mask.shape)\n",
        "        gt_mask = colour_code_segmentation(reverse_one_hot(torch.tensor(gt_mask)), rgb_vals)\n",
        "        \n",
        "        #get IoU score\n",
        "        m = tf.keras.metrics.MeanIoU(num_classes=numm_classes)\n",
        "        m.update_state(gt_mask, pred_mask)\n",
        "        iou = m.result().numpy()\n",
        "        #print(\"MEAN IoU:\" , iou)\n",
        "        ious.append(iou)\n",
        "\n",
        "        #gt_mask = scipy.signal.medfilt(gt_mask, 9)\n",
        "\n",
        "        #Get f1\n",
        "        #m = MultiLabelBinarizer().fit(gt_mask)\n",
        "        #f1 = f1_score(m.transform(gt_mask), m.transform(pred_mask), average=None)\n",
        "        #if len(f1) == num_classes:\n",
        "        #  f1s.append(f1)\n",
        "\n",
        "        metric = MulticlassF1Score(num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        #metric = JaccardIndex(task=\"multiclass\",num_classes=numm_classes, average=None, labels=np.unique(pred_mask) ,validate_args=True)\n",
        "        f1 = metric(torch.tensor(pred_mask), torch.tensor(gt_mask))\n",
        "        if len(f1) == num_classes:\n",
        "          f1[f1 <0.1] = np.nan\n",
        "          print(f1)\n",
        "          f1s.append(np.array(f1))\n",
        "\n",
        "        try:\n",
        "          if idx < 20:\n",
        "            visualize(\n",
        "                original_image = image[0,::],\n",
        "                ground_truth_mask = gt_mask,\n",
        "                predicted_mask = pred_mask,\n",
        "            )\n",
        "        except:\n",
        "          if idx < 20:\n",
        "            visualize(\n",
        "                original_image = image.cuda()[0,::],\n",
        "                ground_truth_mask = gt_mask.cuda(),\n",
        "                predicted_mask = pred_mask.cuda(),\n",
        "            )\n",
        "\n",
        "    \n",
        "    fs1_numpy = np.array(f1s)\n",
        "    av_f1s = np.nanmean(fs1_numpy, axis=0)\n",
        "    #av_f1s = fs1_numpy.mean(axis=0)\n",
        "    av_f1s_av = av_f1s.mean(axis=0)\n",
        "\n",
        "    print (\"Dataset MIoU = \", average(ious))\n",
        "    print (\"Dataset F1 = \", av_f1s)\n",
        "    print (\"Dataset F1 av = \", av_f1s_av)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NUpFShDlbHg6",
        "outputId": "890607b7-e944-41a3-9162-753dd59cc9e2"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"/content/drive/MyDrive/FRESH/label_adapter.pth\", map_location=DEVICE)\n",
        "\n",
        "view_label_predictions(model,val_ds2, num_classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "history_visible": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "interpreter": {
      "hash": "01070a215043f07a8263edac27ed84a056fb8ad0ef2223506d9cf2bf6f4d5c3c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
